#!/usr/bin/env python3
"""
Package experiment results into a zip file for sharing

Creates a comprehensive zip package with:
- Experiment report
- Publication tables
- Raw data
- Quick start guide
"""

import zipfile
from pathlib import Path
from datetime import datetime

# Configuration (fixed paths for moved script)
PROJECT_ROOT = Path(__file__).parent.parent.parent
RESULTS_DIR = PROJECT_ROOT / "results" / "end_to_end"
OUTPUT_ZIP = PROJECT_ROOT / "Multi_Agent_Scheduler_Evaluation_Results.zip"

# Files to include
FILES_TO_PACKAGE = [
    # Main report
    "EXPERIMENT_REPORT.md",

    # Tables
    "tables/table1_performance_comparison.md",
    "tables/table2_dependency_structure.md",
    "tables/table3_scalability_analysis.md",
    "tables/table4_timeout_impact.md",
    "tables/table5_detailed_metrics.md",
    "tables/all_tables_latex.tex",

    # Raw data
    "raw_data/README.md",
    "raw_data/metadata.json",
    "raw_data/summary.csv",
    "raw_data/db_product_sales_comparison.json",
    "raw_data/os_user_analysis_comparison.json",
    "raw_data/os_system_health_fanout_comparison.json",
    "raw_data/web_scraping_fanout_comparison.json",
    "raw_data/data_pipeline_mixed_comparison.json",
]


def create_package_readme():
    """Create a README for the zip package"""
    readme = f"""# Multi-Agent Scheduler - Evaluation Results Package

**Package Date**: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
**Experiment**: Day 7 End-to-End Evaluation
**Framework**: AgentBench Dependency Tasks

---

## ğŸ“¦ Package Contents

This package contains complete experimental results from evaluating a Multi-Agent Scheduler with DAG-based task execution.

### Quick Summary

- **5 test groups** covering different dependency structures (Linear, Fan-out, Mixed DAG)
- **41 total tasks** executed across 2 modes (Sequential vs Hybrid)
- **100% success rate** with 600-second timeout configuration
- **1.23Ã— overall speedup** (Hybrid vs Sequential)

### Key Findings

âœ… **Optimal Threshold**: â‰¥12 tasks required for consistent Hybrid mode advantage
âœ… **Average Speedup**: 1.12Ã— across 5 test groups
âš ï¸ **Small Task Penalty**: 2-8 tasks show DAG scheduling overhead exceeding benefits
âœ… **Best Result**: 1.57Ã— speedup on 3-task linear pipeline
âš ï¸ **Worst Result**: 0.70Ã— (Hybrid slower) on 2-task minimal pipeline

---

## ğŸ“ File Structure

```
Multi_Agent_Scheduler_Evaluation_Results/
â”‚
â”œâ”€â”€ README.md                              â† This file
â”œâ”€â”€ QUICK_START.md                         â† How to use this data
â”œâ”€â”€ EXPERIMENT_REPORT.md                   â† Complete documentation (300+ lines)
â”‚
â”œâ”€â”€ tables/                                 â† 5 Publication Tables
â”‚   â”œâ”€â”€ table1_performance_comparison.md       (Main results)
â”‚   â”œâ”€â”€ table2_dependency_structure.md         (Structure analysis)
â”‚   â”œâ”€â”€ table3_scalability_analysis.md         (Task count trends)
â”‚   â”œâ”€â”€ table4_timeout_impact.md               (60s vs 600s)
â”‚   â”œâ”€â”€ table5_detailed_metrics.md             (Complete data)
â”‚   â””â”€â”€ all_tables_latex.tex                   (LaTeX collection)
â”‚
â””â”€â”€ raw_data/                               â† Raw Experimental Data
    â”œâ”€â”€ README.md                              (Data usage guide)
    â”œâ”€â”€ metadata.json                          (Experiment config)
    â”œâ”€â”€ summary.csv                            (All results in CSV)
    â””â”€â”€ *_comparison.json                      (5 JSON result files)
```

---

## ğŸš€ Quick Start

### 1. Read the Overview
Start with `EXPERIMENT_REPORT.md` for complete analysis.

### 2. Check Key Results
Look at `tables/table1_performance_comparison.md` for main findings.

### 3. Analyze Raw Data

**In Excel/Numbers:**
- Open `raw_data/summary.csv`
- See all 5 test results in spreadsheet format

**In Python:**
```python
import pandas as pd
import json

# Load CSV summary
df = pd.read_csv('raw_data/summary.csv')
print(df[['test_name', 'tasks', 'speedup']])

# Load specific test
with open('raw_data/db_product_sales_comparison.json') as f:
    data = json.load(f)
print(f"Speedup: {{data['hybrid']['speedup']:.2f}}Ã—")
```

**In R:**
```r
data <- read.csv('raw_data/summary.csv')
summary(data$speedup)
plot(data$tasks, data$speedup)
```

### 4. Use in Academic Paper

**LaTeX:**
- Copy table code from `tables/*.md` files
- Each has `\\begin{{table}}...\\end{{table}}` blocks
- Requires `\\usepackage{{booktabs}}`

**Word/Google Docs:**
- Tables are in Markdown (readable format)
- Can copy-paste or convert with Pandoc

---

## ğŸ“Š Experimental Setup

### Test Groups

| Group | Tasks | Structure | Description |
|-------|-------|-----------|-------------|
| db_product_sales | 2 | Linear | Find high-sales products and analyze ratings |
| os_user_analysis | 3 | Linear | Extract users and analyze their files |
| os_system_health_fanout | 8 | Fan-out | System health check with parallel analysis |
| web_scraping_fanout | 12 | Fan-out | Parallel web scraping and aggregation |
| data_pipeline_mixed | 16 | Mixed | Complex multi-stage data processing |

### Execution Modes

1. **Sequential** (Baseline): Execute tasks one by one in dependency order
2. **Hybrid** (DAG Scheduling): Execute independent tasks in parallel batches

### Tools & Configuration

- **MetaAgent**: Claude CLI (Sonnet 4.5) for task decomposition
- **DAG Scheduler**: Kahn's topological sort + async batch execution
- **CLI Executor**: Subprocess execution with 600s timeout
- **Success Detection**: FINAL_ANSWER pattern matching

---

## ğŸ¯ Main Results

### Performance by Test Group (600s Timeout)

| Group | Tasks | Speedup | Best Mode | Notes |
|-------|-------|---------|-----------|-------|
| db_product_sales | 2 | **0.70Ã—** | Sequential | Overhead > benefits |
| os_user_analysis | 3 | **1.57Ã—** | Hybrid | Sweet spot! |
| os_system_health_fanout | 8 | **0.997Ã—** | Sequential | No advantage |
| web_scraping_fanout | 12 | **1.31Ã—** | Hybrid | Clear benefit |
| data_pipeline_mixed | 16 | **1.32Ã—** | Hybrid | Best absolute savings |

### Scalability Trend

- **2-5 tasks**: Overhead dominates â†’ Use Sequential
- **6-10 tasks**: Break-even zone â†’ Case-by-case
- **11+ tasks**: Clear advantage â†’ Use Hybrid

---

## âš ï¸ Important Notes

### Timeout Configuration Critical

- **60s timeout**: Caused 14.6% task failures (6/41 tasks failed)
- **600s timeout**: Achieved 100% success (0 failures)
- **Lesson**: Adequate timeout essential for agent-based systems

### Single-Run Limitation

- Each test group executed **once** (no statistical validation)
- No standard deviation or confidence intervals
- Results show clear trends but need multiple runs for publication

### Recommendations for Future Work

1. Run each test 3-5 times for statistical significance
2. Test with different agent types (Gemini, GPT, etc.)
3. Evaluate on computation-heavy tasks (not just I/O-bound)
4. Implement adaptive timeout based on task characteristics

---

## ğŸ“– Citation

If you use this data in publications:

```
Multi-Agent Scheduler Day 7 End-to-End Evaluation
Date: November 16, 2025
Framework: AgentBench Dependency Tasks
Configuration: 600s timeout, 100% success rate
Total Tasks: 41 (5 groups)
Key Finding: â‰¥12 tasks threshold for DAG scheduling advantage
```

---

## ğŸ“§ Questions?

For questions about this data or methodology:
1. Read `EXPERIMENT_REPORT.md` for detailed analysis
2. Check `raw_data/README.md` for data usage guide
3. Review `tables/` for specific metrics

---

## ğŸ“‚ File Descriptions

### Core Documents

- **README.md** (this file): Package overview
- **QUICK_START.md**: Step-by-step usage guide
- **EXPERIMENT_REPORT.md**: Complete documentation (~300 lines)

### Tables (Markdown + LaTeX)

All tables include both human-readable Markdown and copy-paste-ready LaTeX code.

- **table1_performance_comparison.md**: Main results table (all 5 groups)
- **table2_dependency_structure.md**: Performance by structure type
- **table3_scalability_analysis.md**: Performance by task count range
- **table4_timeout_impact.md**: 60s vs 600s comparison
- **table5_detailed_metrics.md**: Complete experimental data

### Raw Data

- **summary.csv**: All results in spreadsheet format (Excel/Python/R ready)
- **metadata.json**: Experiment configuration and key findings
- **\*_comparison.json**: Individual test results (5 files)

---

**Package Generated**: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
**Total Files**: 15
**Total Size**: ~500KB (mostly text)
**Format**: UTF-8 text files (cross-platform)
"""

    return readme


def create_quick_start():
    """Create a quick start guide"""
    guide = """# Quick Start Guide

This guide helps you quickly navigate and use the evaluation results.

## ğŸ¯ What Do You Want to Do?

### Option 1: Understand the Experiment

**Start here**: `EXPERIMENT_REPORT.md`

- Section 1: Executive Summary (2 minutes)
- Section 2: Experiment Design (5 minutes)
- Section 3: Results Overview (10 minutes)

**Key takeaway**: Multi-agent DAG scheduling needs â‰¥12 tasks to overcome overhead.

---

### Option 2: Get Performance Numbers

**Start here**: `tables/table1_performance_comparison.md`

Quick facts:
- Overall speedup: 1.23Ã—
- Best case: 1.57Ã— (os_user_analysis, 3 tasks)
- Worst case: 0.70Ã— (db_product_sales, 2 tasks)
- Success rate: 100% (41/41 tasks)

---

### Option 3: Analyze Data Yourself

**Start here**: `raw_data/summary.csv`

Open in Excel/Numbers/Google Sheets:
1. Import `summary.csv`
2. Create pivot tables or charts
3. All 5 test results in one file

Python users:
```python
import pandas as pd
df = pd.read_csv('raw_data/summary.csv')
df.plot(x='tasks', y='speedup', kind='scatter')
```

---

### Option 4: Use in Academic Paper

**For LaTeX users**:
1. Go to `tables/table1_performance_comparison.md`
2. Copy the LaTeX code block
3. Paste into your paper
4. Add `\\usepackage{booktabs}` to preamble

**For Word users**:
1. Tables are in Markdown format
2. Copy-paste the table
3. Or convert with Pandoc

**Citation format** (see README.md):
- Cite as: "Multi-Agent Scheduler Evaluation (2025)"
- Reference: Day 7 End-to-End Evaluation
- Dataset: 5 groups, 41 tasks, 100% success

---

## ğŸ“Š File Navigation Map

```
What you want â†’ Where to look

Overall understanding â†’ EXPERIMENT_REPORT.md
Performance numbers â†’ tables/table1_performance_comparison.md
Scalability trends â†’ tables/table3_scalability_analysis.md
Raw data analysis â†’ raw_data/summary.csv
Timeout impact â†’ tables/table4_timeout_impact.md
Complete metrics â†’ tables/table5_detailed_metrics.md
Experiment config â†’ raw_data/metadata.json
Data usage help â†’ raw_data/README.md
```

---

## ğŸš€ 5-Minute Speed Run

1. **Skim** `EXPERIMENT_REPORT.md` Executive Summary (1 min)
2. **Read** `tables/table1_performance_comparison.md` (2 min)
3. **Check** `raw_data/summary.csv` in Excel (2 min)

You now know:
- âœ… What was tested (5 groups, 41 tasks)
- âœ… Main findings (â‰¥12 tasks threshold)
- âœ… Performance metrics (1.23Ã— average speedup)

---

## ğŸ’¡ Tips

### For Researchers
- All text files are UTF-8 encoded (cross-platform)
- JSON files can be loaded in any language
- CSV uses standard format (compatible with Excel/R/Python)

### For Students
- Read Executive Summary first
- Tables have "Key Findings" sections
- Experiment Report has paper writing guide

### For Practitioners
- Check table3 for task scale recommendations
- table4 shows why timeout matters
- metadata.json has tool stack details

---

## â“ Common Questions

**Q: Can I re-run the experiments?**
A: Yes, but requires:
- Claude CLI installed
- AgentBench framework
- ~45 minutes execution time
- See metadata.json for exact config

**Q: What if I need more statistical rigor?**
A: Current data is single-run. For publication:
- Run each test 3-5 times
- Calculate mean/std/confidence intervals
- See "Limitations" in EXPERIMENT_REPORT.md

**Q: Can I use this data in my paper?**
A: Yes! All results are shareable. Please cite appropriately.

**Q: What format are the tables?**
A: Dual format - Markdown (readable) + LaTeX (copy-paste to paper)

**Q: How do I open .md files?**
A:
- Any text editor (Notepad, TextEdit, VS Code)
- Markdown viewer (Typora, Mark Text)
- GitHub/GitLab (renders automatically)

---

**Questions not answered here?**
â†’ Check `EXPERIMENT_REPORT.md` Section 9 (FAQ)
â†’ Review `raw_data/README.md` for data specifics
"""

    return guide


def main():
    """Create zip package"""
    print("\n" + "="*70)
    print("ğŸ“¦ CREATING RESULTS PACKAGE")
    print("="*70 + "\n")

    # Create README files
    print("ğŸ“ Generating package documentation...")
    package_readme = create_package_readme()
    quick_start = create_quick_start()
    print("  âœ“ README.md created")
    print("  âœ“ QUICK_START.md created\n")

    # Create zip file
    print(f"ğŸ“¦ Packaging files into {OUTPUT_ZIP}...")

    with zipfile.ZipFile(OUTPUT_ZIP, 'w', zipfile.ZIP_DEFLATED) as zipf:
        # Add package README and quick start
        zipf.writestr("README.md", package_readme)
        zipf.writestr("QUICK_START.md", quick_start)
        print("  âœ“ Added README.md")
        print("  âœ“ Added QUICK_START.md")

        # Add all experiment files
        for file_rel_path in FILES_TO_PACKAGE:
            file_path = RESULTS_DIR / file_rel_path
            if file_path.exists():
                zipf.write(file_path, file_rel_path)
                print(f"  âœ“ Added {file_rel_path}")
            else:
                print(f"  âœ— Missing {file_rel_path}")

    print()

    # Get zip file size
    zip_size = OUTPUT_ZIP.stat().st_size
    zip_size_kb = zip_size / 1024

    print("âœ… Package created successfully!")
    print(f"\nğŸ“¦ Package Details:")
    print(f"  File: {OUTPUT_ZIP}")
    print(f"  Size: {zip_size_kb:.1f} KB ({zip_size:,} bytes)")
    print(f"  Files: {len(FILES_TO_PACKAGE) + 2} (including README & QUICK_START)")
    print()

    print("ğŸ“§ Ready to share!")
    print(f"  Send {OUTPUT_ZIP} to recipients\n")


if __name__ == "__main__":
    main()
