#!/usr/bin/env python3
"""
End-to-End Multi-Agent Scheduler Evaluation

Complete workflow testing:
1. User inputs complex task
2. MetaAgent decomposes into subtasks
3. Three execution modes comparison:
   - Sequential: Baseline (one by one)
   - Parallel: All tasks simultaneously (ignore dependencies)
   - Hybrid: DAG-based intelligent scheduling

This validates the COMPLETE Day 7 architecture.
"""

import asyncio
import json
import time
from pathlib import Path
from typing import Dict, List, Optional, Any
from datetime import datetime

# Import MetaAgent for task decomposition
# Note: Using simplified version to avoid complex dependencies
from src.meta_agent_simple import SimpleMetaAgentCLI as MetaAgentCLI

# Import DAG Scheduler
from src.orchestration.dag_scheduler import DAGScheduler
from src.orchestration.cli_executor import CLIExecutor
from src.scheduler import Task


def wrap_task_prompt(prompt: str) -> str:
    """
    Enhanced prompt wrapping with stronger FINAL_ANSWER requirement

    CLIExecutor checks for "FINAL_ANSWER:" in output to determine success.
    This function adds a highly visible requirement to ensure Claude outputs it.

    Args:
        prompt: Original task prompt from MetaAgent

    Returns:
        Wrapped prompt with enhanced FINAL_ANSWER instruction
    """
    return f"""{prompt}

{'='*70}
ğŸš¨ CRITICAL REQUIREMENT - DO NOT SKIP ğŸš¨
{'='*70}

After completing this task, you MUST output EXACTLY this line:

    FINAL_ANSWER: OK

This is REQUIRED for task verification. Without this line, the task
will be marked as FAILED even if you completed it correctly.

DO NOT FORGET TO OUTPUT "FINAL_ANSWER: OK" AT THE END!

{'='*70}"""


class EndToEndEvaluator:
    """
    Complete end-to-end evaluation of the multi-agent scheduler

    Workflow:
    User Input â†’ MetaAgent Decomposition â†’ Mode Execution â†’ Metrics Collection
    """

    def __init__(self):
        """Initialize evaluator with MetaAgent and Scheduler"""
        print("ğŸ”§ Initializing End-to-End Evaluator...")

        # MetaAgent for task decomposition
        self.meta_agent = MetaAgentCLI(agent_type='claude')
        print("  âœ“ MetaAgent initialized (using Claude CLI)")

        # CLI Executor (needed by DAG Scheduler)
        # Use 600s timeout to avoid premature task failures
        self.cli_executor = CLIExecutor(timeout=600.0)
        print("  âœ“ CLI Executor created with 600s timeout (will initialize async)")

        # DAG Scheduler for hybrid mode (needs executor)
        self.dag_scheduler = DAGScheduler(
            executor=self.cli_executor,
            default_agent="claude",
            verbose=True,
            use_meta_agent=False  # We handle decomposition separately
        )
        print("  âœ“ DAG Scheduler initialized")

        # Results storage
        self.results_dir = Path("results/end_to_end")
        self.results_dir.mkdir(parents=True, exist_ok=True)

        print("âœ… Evaluator ready\n")

    async def decompose_task(self, user_input: str, complexity: str) -> List[Task]:
        """
        Step 1: Decompose user task into subtasks using MetaAgent

        Args:
            user_input: User's complex task description
            complexity: Task complexity ('simple', 'medium', 'complex')

        Returns:
            List of Task objects with dependencies
        """
        # Initialize executor on first use (if not already initialized)
        if not hasattr(self.cli_executor, '_initialized') or not self.cli_executor._initialized:
            await self.cli_executor.initialize()
            print("  âœ“ CLI Executor initialized\n")

        print(f"\n{'='*70}")
        print(f"ğŸ§  STEP 1: Task Decomposition")
        print(f"{'='*70}")
        print(f"ğŸ“ User Input: {user_input}")
        print(f"ğŸ“Š Complexity: {complexity.upper()}")
        print()

        start_time = time.time()

        # Determine task count based on complexity
        task_ranges = {
            'simple': (5, 8),
            'medium': (10, 15),
            'complex': (15, 20)
        }
        min_tasks, max_tasks = task_ranges.get(complexity, (10, 15))

        print(f"ğŸ” Expected subtasks: {min_tasks}-{max_tasks}")
        print("â³ Decomposing with AI (this may take 30-60 seconds)...\n")

        try:
            # Call MetaAgent to decompose task
            tasks = await self.meta_agent.decompose_task(
                user_input,
                min_tasks=min_tasks,
                max_tasks=max_tasks,
                use_dynamic_complexity=True
            )

            decompose_time = time.time() - start_time

            print(f"\nâœ… Decomposition complete!")
            print(f"  ğŸ“¦ Generated {len(tasks)} subtasks")
            print(f"  â±ï¸  Time: {decompose_time:.2f}s")

            # Display task tree
            self._print_task_tree(tasks)

            # Save decomposition result
            self._save_decomposition(user_input, complexity, tasks, decompose_time)

            return tasks

        except Exception as e:
            print(f"\nâŒ Decomposition failed: {e}")
            print("   Using fallback: single task")
            return [Task(
                id="task1",
                prompt=user_input,
                task_type="general",
                priority=1
            )]

    async def run_sequential(self, tasks: List[Task]) -> Dict[str, Any]:
        """
        Mode 1: Sequential execution (baseline)

        Execute tasks one by one, respecting dependencies
        """
        print(f"\n{'='*70}")
        print("â­ï¸  MODE 1: Sequential Execution (Baseline)")
        print(f"{'='*70}")

        start_time = time.time()
        results = []
        failed_count = 0

        print(f"ğŸ“‹ Executing {len(tasks)} tasks sequentially...\n")

        # Build dependency map for correct ordering
        task_map = {task.id: task for task in tasks}
        executed = set()

        def can_execute(task):
            return all(dep in executed for dep in task.depends_on)

        # Execute in dependency order
        while len(executed) < len(tasks):
            ready_tasks = [t for t in tasks if t.id not in executed and can_execute(t)]

            if not ready_tasks:
                print("âš ï¸  Warning: Circular dependency detected or no executable tasks")
                break

            for task in ready_tasks:
                print(f"  ğŸ”„ [{len(executed)+1}/{len(tasks)}] Executing: {task.id}")

                try:
                    # Wrap prompt to include FINAL_ANSWER requirement
                    wrapped_prompt = wrap_task_prompt(task.prompt)

                    result = await self.cli_executor.execute(
                        task_prompt=wrapped_prompt,
                        agent_name="claude",
                        task_id=task.id
                    )
                    results.append(result)
                    executed.add(task.id)

                    status = "âœ“" if result.success else "âœ—"
                    latency = result.latency
                    print(f"     {status} Completed in {latency:.2f}s")

                    if not result.success:
                        failed_count += 1

                except Exception as e:
                    print(f"     âœ— Failed: {e}")
                    failed_count += 1
                    executed.add(task.id)

        total_time = time.time() - start_time
        success_rate = ((len(tasks) - failed_count) / len(tasks)) * 100

        print(f"\nğŸ“Š Sequential Results:")
        print(f"  Total tasks: {len(tasks)}")
        print(f"  Completed: {len(tasks) - failed_count}")
        print(f"  Failed: {failed_count}")
        print(f"  Success rate: {success_rate:.1f}%")
        print(f"  Total time: {total_time:.2f}s")
        print(f"  Avg latency: {total_time/len(tasks):.2f}s per task")

        return {
            "mode": "sequential",
            "total_tasks": len(tasks),
            "completed": len(tasks) - failed_count,
            "failed": failed_count,
            "success_rate": success_rate,
            "total_time": total_time,
            "avg_latency": total_time / len(tasks),
            "results": results
        }

    async def run_parallel(self, tasks: List[Task]) -> Dict[str, Any]:
        """
        Mode 2: Parallel execution (no dependency handling)

        Execute all tasks simultaneously, ignoring dependencies
        Expected: Some tasks will fail due to missing dependencies
        """
        print(f"\n{'='*70}")
        print("âš¡ MODE 2: Parallel Execution (No Dependency Handling)")
        print(f"{'='*70}")
        print("âš ï¸  Warning: This mode ignores dependencies - failures expected!\n")

        start_time = time.time()

        print(f"ğŸ“‹ Launching {len(tasks)} tasks in parallel...\n")

        # Execute all tasks simultaneously
        async def execute_with_info(task, idx):
            print(f"  ğŸš€ [{idx+1}/{len(tasks)}] Launching: {task.id}")
            try:
                # Wrap prompt to include FINAL_ANSWER requirement
                wrapped_prompt = wrap_task_prompt(task.prompt)

                result = await self.cli_executor.execute(
                    task_prompt=wrapped_prompt,
                    agent_name="claude",
                    task_id=task.id
                )
                status = "âœ“" if result.success else "âœ—"
                print(f"     {status} {task.id} completed")
                return result
            except Exception as e:
                print(f"     âœ— {task.id} failed: {e}")
                # Return a mock result object with success=False
                from dataclasses import dataclass
                @dataclass
                class FailedResult:
                    success: bool = False
                    error: str = str(e)
                    latency: float = 0.0
                return FailedResult()

        results = await asyncio.gather(*[
            execute_with_info(task, idx)
            for idx, task in enumerate(tasks)
        ])

        total_time = time.time() - start_time
        failed_count = sum(1 for r in results if not r.success)
        success_rate = ((len(tasks) - failed_count) / len(tasks)) * 100

        print(f"\nğŸ“Š Parallel Results:")
        print(f"  Total tasks: {len(tasks)}")
        print(f"  Completed: {len(tasks) - failed_count}")
        print(f"  Failed: {failed_count}")
        print(f"  Success rate: {success_rate:.1f}%")
        print(f"  Total time: {total_time:.2f}s")
        print(f"  Avg latency: {total_time/len(tasks):.2f}s per task")
        print(f"  âš ï¸  Note: Failures expected due to ignored dependencies")

        return {
            "mode": "parallel",
            "total_tasks": len(tasks),
            "completed": len(tasks) - failed_count,
            "failed": failed_count,
            "success_rate": success_rate,
            "total_time": total_time,
            "avg_latency": total_time / len(tasks),
            "results": results
        }

    async def run_hybrid(self, tasks: List[Task]) -> Dict[str, Any]:
        """
        Mode 3: Hybrid execution (DAG-based intelligent scheduling)

        Use DAG scheduler with dependency handling and batch parallelization
        """
        print(f"\n{'='*70}")
        print("ğŸ¯ MODE 3: Hybrid Execution (DAG Scheduler)")
        print(f"{'='*70}")
        print("âœ¨ Using intelligent scheduling with dependency handling\n")

        start_time = time.time()

        print(f"ğŸ“‹ Scheduling {len(tasks)} tasks with DAG...\n")

        # Wrap all task prompts to include FINAL_ANSWER requirement
        # Create new Task objects with wrapped prompts (don't modify originals)
        from src.scheduler import Task
        wrapped_tasks = []
        for task in tasks:
            wrapped_task = Task(
                id=task.id,
                prompt=wrap_task_prompt(task.prompt),
                task_type=task.task_type,
                depends_on=task.depends_on,
                priority=task.priority,
                metadata=task.metadata
            )
            wrapped_tasks.append(wrapped_task)

        # Use DAG scheduler
        dag_result = await self.dag_scheduler.execute_dag(
            wrapped_tasks,
            agent_mapping=None  # Use default agent
        )

        total_time = time.time() - start_time

        # Build statistics from DAGResult attributes
        success_rate = (dag_result.success_count / dag_result.task_count * 100) if dag_result.task_count > 0 else 0
        avg_latency = total_time / dag_result.task_count if dag_result.task_count > 0 else 0

        stats = {
            'total_tasks': dag_result.task_count,
            'completed_tasks': dag_result.success_count,
            'failed_tasks': dag_result.failed_count,
            'success_rate': success_rate,
            'batch_count': dag_result.batch_count,
            'avg_latency': avg_latency
        }

        print(f"\nğŸ“Š Hybrid Results:")
        print(f"  Total tasks: {stats['total_tasks']}")
        print(f"  Completed: {stats['completed_tasks']}")
        print(f"  Failed: {stats['failed_tasks']}")
        print(f"  Success rate: {stats['success_rate']:.1f}%")
        print(f"  Total time: {total_time:.2f}s")
        print(f"  Batches: {stats['batch_count']}")
        print(f"  Avg latency: {stats['avg_latency']:.2f}s per task")

        return {
            "mode": "hybrid",
            "total_time": total_time,
            **stats,
            "dag_result": dag_result
        }

    async def run_complete_evaluation(
        self,
        user_input: str,
        complexity: str,
        test_name: str
    ) -> Dict[str, Any]:
        """
        Run complete end-to-end evaluation for one test case

        Steps:
        1. Decompose task with MetaAgent
        2. Run Sequential mode
        3. Run Parallel mode
        4. Run Hybrid mode
        5. Compare results
        """
        print(f"\n{'#'*70}")
        print(f"# END-TO-END EVALUATION: {test_name.upper()}")
        print(f"{'#'*70}")

        # Step 1: Decompose
        tasks = await self.decompose_task(user_input, complexity)

        if len(tasks) == 0:
            print("âŒ No tasks generated, aborting evaluation")
            return None

        # Step 2-4: Run three modes
        sequential_result = await self.run_sequential(tasks.copy())
        parallel_result = await self.run_parallel(tasks.copy())
        hybrid_result = await self.run_hybrid(tasks.copy())

        # Step 5: Compare and save
        comparison = self._generate_comparison(
            test_name,
            user_input,
            complexity,
            tasks,
            sequential_result,
            parallel_result,
            hybrid_result
        )

        # Save results
        self._save_results(test_name, comparison)

        return comparison

    def _generate_comparison(
        self,
        test_name: str,
        user_input: str,
        complexity: str,
        tasks: List[Task],
        sequential: Dict,
        hybrid: Dict,
        parallel: Dict = None  # Optional - removed from experiments
    ) -> Dict[str, Any]:
        """Generate comparison report (Sequential vs Hybrid)"""

        print(f"\n{'='*70}")
        print("ğŸ“ˆ COMPARISON SUMMARY")
        print(f"{'='*70}")

        # Calculate speedups
        seq_time = sequential['total_time']
        hyb_time = hybrid['total_time']
        hyb_speedup = seq_time / hyb_time if hyb_time > 0 else 0

        print(f"\nğŸ† Performance Comparison:")
        print(f"  Sequential: {seq_time:.2f}s (baseline)")
        print(f"  Hybrid:     {hyb_time:.2f}s ({hyb_speedup:.2f}x speedup)")

        print(f"\nâœ… Success Rate Comparison:")
        print(f"  Sequential: {sequential['success_rate']:.1f}%")
        print(f"  Hybrid:     {hybrid.get('success_rate', 0):.1f}%")

        print(f"\nğŸ¯ Best Mode: ", end="")
        seq_success = sequential['success_rate']
        hyb_success = hybrid.get('success_rate', 0)

        if hyb_speedup > 1.0 and hyb_success >= seq_success:
            print(f"Hybrid ({hyb_speedup:.2f}x faster with {hyb_success:.0f}% success)")
        elif hyb_success > seq_success:
            print(f"Hybrid (higher success rate: {hyb_success:.0f}% vs {seq_success:.0f}%)")
        elif hyb_speedup > 1.0:
            print(f"Hybrid ({hyb_speedup:.2f}x faster)")
        else:
            print(f"Sequential (baseline - limited parallelization opportunity)")

        result = {
            "test_name": test_name,
            "user_input": user_input,
            "complexity": complexity,
            "task_count": len(tasks),
            "timestamp": datetime.now().isoformat(),
            "modes": {
                "sequential": sequential,
                "hybrid": hybrid
            },
            "speedups": {
                "hybrid_vs_sequential": hyb_speedup
            }
        }

        # Include parallel if provided (for backward compatibility)
        if parallel is not None:
            par_time = parallel['total_time']
            par_speedup = seq_time / par_time if par_time > 0 else 0
            result["modes"]["parallel"] = parallel
            result["speedups"]["parallel_vs_sequential"] = par_speedup

        return result

    def _print_task_tree(self, tasks: List[Task]):
        """Print task dependency tree"""
        print(f"\nğŸ“‹ Task Decomposition Tree:")
        print("â”€" * 70)
        for i, task in enumerate(tasks):
            symbol = "â””â”€" if i == len(tasks) - 1 else "â”œâ”€"
            deps = f" [depends on: {', '.join(task.depends_on)}]" if task.depends_on else ""
            print(f"{symbol} {task.id}: {task.prompt[:60]}...{deps}")
        print("â”€" * 70)

    def _save_decomposition(
        self,
        user_input: str,
        complexity: str,
        tasks: List[Task],
        decompose_time: float
    ):
        """Save task decomposition for reuse"""
        filename = self.results_dir / f"decomposition_{complexity}.json"

        data = {
            "user_input": user_input,
            "complexity": complexity,
            "decompose_time": decompose_time,
            "task_count": len(tasks),
            "tasks": [
                {
                    "id": task.id,
                    "prompt": task.prompt,
                    "task_type": task.task_type,
                    "priority": task.priority,
                    "depends_on": task.depends_on
                }
                for task in tasks
            ]
        }

        with open(filename, 'w') as f:
            json.dump(data, f, indent=2)

        print(f"\nğŸ’¾ Decomposition saved to: {filename}")

    def _convert_to_serializable(self, obj):
        """
        Convert TaskResult objects to JSON-serializable dicts

        Recursively handles nested structures (dicts, lists).
        """
        # Import TaskResult here to avoid circular import
        try:
            from src.orchestration.cli_executor import TaskResult
        except ImportError:
            TaskResult = None

        if TaskResult and isinstance(obj, TaskResult):
            return {
                "task_id": obj.task_id,
                "success": obj.success,
                "output": obj.output[:200] if obj.output else "",  # Truncate output
                "latency": obj.latency,
                "agent": obj.agent,
                "error": obj.error
            }
        elif isinstance(obj, dict):
            return {k: self._convert_to_serializable(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [self._convert_to_serializable(item) for item in obj]
        else:
            return obj

    def _save_results(self, test_name: str, comparison: Dict):
        """Save complete evaluation results (Sequential vs Hybrid)"""
        filename = self.results_dir / f"{test_name}_comparison.json"

        # Extract only key performance metrics (avoid complex object serialization)
        summary = {
            "test_name": test_name,
            "user_input": comparison.get("user_input", ""),
            "complexity": comparison.get("complexity", ""),
            "task_count": comparison.get("task_count", 0),
            "sequential": {
                "total_time": comparison.get("modes", {}).get("sequential", {}).get("total_time", 0),
                "success_rate": comparison.get("modes", {}).get("sequential", {}).get("success_rate", 0),
                "completed": comparison.get("modes", {}).get("sequential", {}).get("completed", 0),
                "failed": comparison.get("modes", {}).get("sequential", {}).get("failed", 0)
            },
            "hybrid": {
                "total_time": comparison.get("modes", {}).get("hybrid", {}).get("total_time", 0),
                "success_rate": comparison.get("modes", {}).get("hybrid", {}).get("success_rate", 0),
                "completed": comparison.get("modes", {}).get("hybrid", {}).get("completed", 0),
                "failed": comparison.get("modes", {}).get("hybrid", {}).get("failed", 0),
                "batches": comparison.get("modes", {}).get("hybrid", {}).get("batches", 0),
                "speedup": comparison.get("speedups", {}).get("hybrid_vs_sequential", 0)
            },
            "best_mode": "Hybrid" if comparison.get("speedups", {}).get("hybrid_vs_sequential", 0) > 1.0 else "Sequential",
            "timestamp": comparison.get("timestamp", datetime.now().isoformat())
        }

        with open(filename, 'w') as f:
            json.dump(summary, f, indent=2)

        print(f"\nğŸ’¾ Results saved to: {filename}")


def load_agentbench_tasks(group_id: Optional[str] = None) -> List[Dict]:
    """
    Load dependency tasks from AgentBench benchmark

    Args:
        group_id: Optional specific group to load. If None, loads all groups.

    Returns:
        List of dependency groups from AgentBench
    """
    agentbench_file = Path(__file__).parent / "AgentBench" / "dependency_tasks.json"

    if not agentbench_file.exists():
        raise FileNotFoundError(f"AgentBench tasks not found: {agentbench_file}")

    with open(agentbench_file, 'r') as f:
        data = json.load(f)

    groups = data.get("groups", [])

    if group_id:
        groups = [g for g in groups if g["group_id"] == group_id]
        if not groups:
            raise ValueError(f"Group '{group_id}' not found in AgentBench")

    return groups


def convert_agentbench_to_tasks(group: Dict) -> List[Task]:
    """
    Convert AgentBench dependency group to Task objects

    Args:
        group: Dependency group from AgentBench JSON

    Returns:
        List of Task objects with dependencies
    """
    tasks = []
    for task_def in group["tasks"]:
        # Wrap prompt with FINAL_ANSWER requirement
        wrapped_prompt = wrap_task_prompt(task_def["prompt"])

        task = Task(
            id=task_def["id"],
            prompt=wrapped_prompt,
            task_type=task_def.get("type", "general"),
            depends_on=task_def.get("depends_on", []),
            metadata={
                "description": task_def["description"],
                "agentbench_group": group["group_id"],
                "agent": "claude"
            }
        )
        tasks.append(task)

    return tasks


async def main():
    """
    Main evaluation workflow

    Uses AgentBench dependency tasks for evaluation:
    - Load dependency groups from AgentBench/dependency_tasks.json
    - Execute with 2 modes: Sequential (baseline), Hybrid (DAG scheduling)
    - Compare performance and success rates across different dependency structures
    """
    print("\n" + "="*70)
    print("ğŸš€ MULTI-AGENT SCHEDULER - END-TO-END EVALUATION")
    print("="*70)
    print("\nThis evaluation validates the complete Day 7 architecture:")
    print("  AgentBench Tasks â†’ 2 Execution Modes â†’ Performance Comparison\n")

    # Disable global configs during test
    print("ğŸ”§ Step 0: Disabling global configurations...")
    config_files = [
        (Path.home() / ".claude" / "CLAUDE.md", Path.home() / ".claude" / "CLAUDE.md.test_backup"),
        (Path.home() / ".gemini" / "GEMINI.md", Path.home() / ".gemini" / "GEMINI.md.test_backup"),
        (Path(__file__).parent / "AGENTS.md", Path(__file__).parent / "AGENTS.md.test_backup"),
    ]

    disabled_configs = []
    for config_file, backup_file in config_files:
        if config_file.exists() and not backup_file.exists():
            config_file.rename(backup_file)
            disabled_configs.append((config_file, backup_file))
            print(f"  âœ“ Disabled {config_file.name}")

    print()

    # Load AgentBench dependency groups
    print("ğŸ“š Loading AgentBench dependency tasks...")
    all_groups = load_agentbench_tasks()
    print(f"  âœ“ Loaded {len(all_groups)} dependency groups")
    print(f"  ğŸ“Š Total tasks: {sum(len(g['tasks']) for g in all_groups)}\n")

    # Initialize evaluator
    evaluator = EndToEndEvaluator()

    # Select test groups covering different dependency structures
    test_groups = [
        # Linear dependency groups (baseline)
        "db_product_sales",           # 2 tasks: Linear chain (Aâ†’B)
        "os_user_analysis",            # 3 tasks: Linear chain (Aâ†’Bâ†’C)

        # Fan-out dependency groups (demonstrate Hybrid advantages)
        "os_system_health_fanout",     # 8 tasks: Aâ†’[B1-B4]â†’[C1,C2]â†’D (4 batches)
        "web_scraping_fanout",         # 12 tasks: Aâ†’[B1-B6]â†’[C1-C3]â†’Dâ†’E (5 batches)

        # Mixed DAG (complex real-world scenario)
        "data_pipeline_mixed",         # 16 tasks: Multi-level mixed dependencies (6 batches)
    ]

    try:
        # Run evaluations on AgentBench dependency groups
        print(f"\nğŸ“‹ Running {len(test_groups)} AgentBench dependency groups...\n")

        results = []
        for group_id in test_groups:
            # Find the group definition
            group = next(g for g in all_groups if g["group_id"] == group_id)

            print(f"\n{'='*70}")
            print(f"ğŸ“¦ Testing Group: {group_id}")
            print(f"{'='*70}")
            print(f"ğŸ“ Description: {group['description']}")
            print(f"ğŸ“Š Tasks: {len(group['tasks'])}")
            print()

            # Convert to Task objects (SKIP MetaAgent decomposition)
            tasks = convert_agentbench_to_tasks(group)

            # Display task tree
            print("ğŸ“‹ Task Dependency Tree:")
            for task in tasks:
                indent = "  " * len(task.depends_on)
                deps = f" (depends on: {', '.join(task.depends_on)})" if task.depends_on else " (root task)"
                description = task.metadata.get("description", "")
                print(f"{indent}â””â”€ {task.id}: {description}{deps}")
            print()

            # Initialize executor on first use
            if not hasattr(evaluator.cli_executor, '_initialized') or not evaluator.cli_executor._initialized:
                await evaluator.cli_executor.initialize()
                print("  âœ“ CLI Executor initialized\n")

            # Execute with 2 modes (Sequential baseline, Hybrid DAG scheduling)
            print(f"\nğŸ¯ Running 2 execution modes...\n")

            sequential_result = await evaluator.run_sequential(tasks.copy())
            hybrid_result = await evaluator.run_hybrid(tasks.copy())

            # Generate comparison report
            comparison = evaluator._generate_comparison(
                test_name=group_id,
                user_input=f"AgentBench: {group['description']}",
                complexity="agentbench",
                tasks=tasks,
                sequential=sequential_result,
                hybrid=hybrid_result
            )

            # Save results
            evaluator._save_results(group_id, comparison)

            results.append({
                "group_id": group_id,
                "comparison": comparison
            })

            # Pause between tests
            if group_id != test_groups[-1]:  # Don't pause after last test
                print(f"\nâ¸ï¸  Pausing 10 seconds before next group...\n")
                await asyncio.sleep(10)

        # Final summary
        print(f"\n{'#'*70}")
        print("# AGENTBENCH EVALUATION COMPLETE")
        print(f"{'#'*70}")
        print(f"\nâœ… All {len(test_groups)} dependency groups completed!")
        print(f"ğŸ“Š Results saved in: results/end_to_end/")

        # Print summary table
        print(f"\n{'='*70}")
        print("PERFORMANCE SUMMARY")
        print(f"{'='*70}\n")
        print(f"{'Group ID':<25} {'Sequential':<12} {'Parallel':<12} {'Hybrid':<12}")
        print("-" * 70)
        for result in results:
            group_id = result["group_id"]
            comp = result["comparison"]
            seq_time = comp.get("sequential", {}).get("total_time", 0)
            par_time = comp.get("parallel", {}).get("total_time", 0)
            hyb_time = comp.get("hybrid", {}).get("total_time", 0)
            print(f"{group_id:<25} {seq_time:>10.2f}s  {par_time:>10.2f}s  {hyb_time:>10.2f}s")
        print()

    finally:
        # Restore global configs
        print(f"\nğŸ”§ Restoring global configurations...")
        for config_file, backup_file in disabled_configs:
            if backup_file.exists():
                backup_file.rename(config_file)
                print(f"  âœ“ Restored {config_file.name}")

        print("\nâœ¨ Evaluation finished!\n")


if __name__ == "__main__":
    asyncio.run(main())
