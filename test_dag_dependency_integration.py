"""
DAG + Dependency Injection Integration Test

Tests the complete workflow:
1. AgentBenchLoader: Load dependency task groups
2. DAGScheduler: Topological sort and batch execution
3. DependencyInjector: Inject upstream outputs into downstream inputs
4. CLIExecutor: Execute tasks via Claude CLI

Two modes:
- Mock mode: Fast unit test with simulated task results
- CLI mode: Full integration test with actual CLI execution
"""

import sys
import asyncio
from pathlib import Path
from typing import Dict, Any

# Add src to path
sys.path.insert(0, str(Path(__file__).parent / "src"))
from scheduler import Task

# Import orchestration modules
sys.path.insert(0, str(Path(__file__).parent / "src" / "orchestration"))
from agentbench_loader import AgentBenchLoader
from dag_scheduler import DAGScheduler
from dependency_injector import DependencyInjector
from cli_executor import CLIExecutor
from executor import TaskResult


# ============================================================================
# Mock Executor for Fast Testing
# ============================================================================

class MockExecutorWithData:
    """
    Mock executor that simulates task execution with realistic parsed_data

    This allows testing the dependency injection logic without
    actually running tasks via CLI.
    """

    def __init__(self):
        self.initialized = True
        self.call_count = 0

        # Predefined mock results for os_user_analysis group
        self.mock_results = {
            "os_dep_1a": TaskResult(
                task_id="os_dep_1a",
                success=True,
                output="Found 5 users: root, alex, mysql, postgres, nginx",
                parsed_data={
                    "users": ["root", "alex", "mysql", "postgres", "nginx"],
                    "user_count": 5
                },
                agent="mock"
            ),
            "os_dep_1b": TaskResult(
                task_id="os_dep_1b",
                success=True,
                output="Found 42 files in /home/alex",
                parsed_data={
                    "count": 42,
                    "directory": "/home/alex"
                },
                agent="mock"
            ),
            "os_dep_1c": TaskResult(
                task_id="os_dep_1c",
                success=True,
                output="Yes, 42 files exceeds 10 files",
                parsed_data={
                    "exceeds_threshold": True,
                    "threshold": 10,
                    "actual_count": 42
                },
                agent="mock"
            )
        }

    async def initialize(self, **kwargs):
        """Initialize (no-op for mock)"""
        pass

    async def execute(
        self,
        task_prompt: str,
        agent_name: str,
        task_id: str,
        **kwargs
    ) -> TaskResult:
        """
        Simulate task execution with predefined results

        For testing dependency injection, we verify that:
        1. The enhanced prompt contains injected parameters
        2. The mock result includes parsed_data for next task
        """
        self.call_count += 1

        # Check if enhanced prompt was used (contains injected params)
        has_injection = "CONTEXT FROM UPSTREAM TASKS" in task_prompt

        if has_injection:
            print(f"  ‚úì Task {task_id} received enhanced prompt with dependency injection")
            # Show injected params
            if "target_user" in task_prompt:
                print(f"    - Found injected param: target_user")
            if "file_count" in task_prompt:
                print(f"    - Found injected param: file_count")

        # Return predefined mock result
        if task_id in self.mock_results:
            result = self.mock_results[task_id]
            print(f"  ‚úì Task {task_id} completed (mock)")
            return result
        else:
            # Generic mock result for unknown tasks
            return TaskResult(
                task_id=task_id,
                success=True,
                output="Mock task completed",
                parsed_data={"result": "mock_value"},
                agent="mock"
            )

    async def shutdown(self):
        """Shutdown (no-op for mock)"""
        pass


# ============================================================================
# Test Cases
# ============================================================================

async def test_mock_mode_os_user_analysis():
    """
    Test 1: Mock mode - os_user_analysis group

    Validates:
    - AgentBenchLoader loads tasks correctly
    - DAGScheduler creates correct batches
    - DependencyInjector enhances prompts
    - Mock executor receives enhanced prompts
    """
    print("\n" + "=" * 70)
    print("TEST 1: Mock Mode - OS User Analysis (3-task chain)")
    print("=" * 70)

    try:
        # Load task group
        loader = AgentBenchLoader()
        tasks = loader.get_group_tasks("os_user_analysis")
        input_mappings = loader.get_input_mappings("os_user_analysis")

        print(f"\n‚úì Loaded {len(tasks)} tasks")
        for task in tasks:
            deps = f" (depends on: {', '.join(task.depends_on)})" if task.depends_on else ""
            print(f"  - {task.id}{deps}")

        print(f"\n‚úì Input mappings:")
        for task_id, mapping in input_mappings.items():
            print(f"  {task_id}: {mapping}")

        # Create DAG scheduler with mock executor
        executor = MockExecutorWithData()
        scheduler = DAGScheduler(executor, verbose=True)

        print(f"\nüöÄ Executing DAG with dependency injection...")

        # Execute DAG with dependency injection
        result = await scheduler.execute_dag(
            tasks=tasks,
            agent_mapping={task.id: "mock" for task in tasks},
            input_mappings=input_mappings,
            extract_data=True  # Enable data extraction
        )

        print(f"\nüìä Execution Results:")
        print(f"  Success: {result.success}")
        print(f"  Total tasks: {result.total_tasks}")
        print(f"  Completed: {result.completed_tasks}")
        print(f"  Failed: {result.failed_tasks}")
        print(f"  Total time: {result.total_time:.2f}s")
        print(f"  Batches: {result.batch_count}")

        # Validate results
        passed = True

        if not result.success:
            print("\n‚ùå DAG execution failed")
            passed = False
        else:
            print("\n‚úÖ DAG execution succeeded")

        # Check that all tasks completed
        if result.completed_tasks != len(tasks):
            print(f"‚ùå Expected {len(tasks)} completed tasks, got {result.completed_tasks}")
            passed = False
        else:
            print(f"‚úÖ All {len(tasks)} tasks completed")

        # Check that dependency injection was used
        if executor.call_count != len(tasks):
            print(f"‚ùå Expected {len(tasks)} executor calls, got {executor.call_count}")
            passed = False
        else:
            print(f"‚úÖ All tasks executed with correct call count")

        # Validate task results contain parsed_data
        print(f"\nüì¶ Checking parsed_data in results:")
        for task_id, task_result in result.task_results.items():
            if task_result.parsed_data:
                print(f"  ‚úì {task_id}: {list(task_result.parsed_data.keys())}")
            else:
                print(f"  ‚ö†Ô∏è  {task_id}: No parsed_data")

        if passed:
            print("\n‚úÖ TEST PASSED: Mock mode integration test")
        else:
            print("\n‚ùå TEST FAILED: Mock mode integration test")

        return passed

    except Exception as e:
        print(f"\n‚ùå TEST FAILED: {e}")
        import traceback
        traceback.print_exc()
        return False


async def test_mock_mode_db_product_sales():
    """
    Test 2: Mock mode - db_product_sales group (2-task chain)
    """
    print("\n" + "=" * 70)
    print("TEST 2: Mock Mode - DB Product Sales (2-task chain)")
    print("=" * 70)

    try:
        # Load task group
        loader = AgentBenchLoader()
        tasks = loader.get_group_tasks("db_product_sales")
        input_mappings = loader.get_input_mappings("db_product_sales")

        print(f"\n‚úì Loaded {len(tasks)} tasks")

        # Create mock results for this group
        class MockDBExecutor(MockExecutorWithData):
            def __init__(self):
                super().__init__()
                self.mock_results = {
                    "db_dep_1a": TaskResult(
                        task_id="db_dep_1a",
                        success=True,
                        output="Found 8 products with sales > 1000",
                        parsed_data={
                            "product_ids": [101, 102, 105, 108, 110, 115, 120, 125],
                            "product_count": 8
                        },
                        agent="mock"
                    ),
                    "db_dep_1b": TaskResult(
                        task_id="db_dep_1b",
                        success=True,
                        output="Average rating is 4.3",
                        parsed_data={
                            "avg_rating": 4.3,
                            "total_reviews": 245
                        },
                        agent="mock"
                    )
                }

        executor = MockDBExecutor()
        scheduler = DAGScheduler(executor, verbose=False)

        # Execute
        result = await scheduler.execute_dag(
            tasks=tasks,
            agent_mapping={task.id: "mock" for task in tasks},
            input_mappings=input_mappings,
            extract_data=True
        )

        # Validate
        passed = result.success and result.completed_tasks == len(tasks)

        if passed:
            print(f"\n‚úÖ TEST PASSED: DB Product Sales")
            print(f"  - {result.completed_tasks} tasks completed")
            print(f"  - {result.batch_count} batches executed")
        else:
            print(f"\n‚ùå TEST FAILED")

        return passed

    except Exception as e:
        print(f"\n‚ùå TEST FAILED: {e}")
        import traceback
        traceback.print_exc()
        return False


async def test_dependency_injection_validation():
    """
    Test 3: Validate that dependency injection actually works

    This test explicitly checks that:
    1. Upstream parsed_data is extracted correctly
    2. Path expressions are resolved correctly
    3. Enhanced prompts contain injected values
    """
    print("\n" + "=" * 70)
    print("TEST 3: Dependency Injection Validation")
    print("=" * 70)

    try:
        # Create simple 2-task chain
        task_a = Task(
            id="task_a",
            prompt="List all users",
            depends_on=[]
        )

        task_b = Task(
            id="task_b",
            prompt="Count files for user: {target_user}",
            depends_on=["task_a"]
        )

        tasks = [task_a, task_b]
        input_mappings = {
            "task_b": {
                "target_user": "task_a.users[0]",
                "total_users": "task_a.user_count"
            }
        }

        # Create mock executor that captures enhanced prompts
        captured_prompts = {}

        class PromptCapturingExecutor(MockExecutorWithData):
            def __init__(self):
                super().__init__()
                # Override mock results for test_dependency_injection_validation
                self.mock_results = {
                    "task_a": TaskResult(
                        task_id="task_a",
                        success=True,
                        output="Found users: root, alice, bob",
                        parsed_data={
                            "users": ["root", "alice", "bob"],
                            "user_count": 3
                        },
                        agent="mock"
                    ),
                    "task_b": TaskResult(
                        task_id="task_b",
                        success=True,
                        output="Found 25 files",
                        parsed_data={"count": 25},
                        agent="mock"
                    )
                }

            async def execute(self, task_prompt, agent_name, task_id, **kwargs):
                captured_prompts[task_id] = task_prompt
                return await super().execute(task_prompt, agent_name, task_id, **kwargs)

        executor = PromptCapturingExecutor()
        scheduler = DAGScheduler(executor, verbose=False)

        # Execute
        result = await scheduler.execute_dag(
            tasks=tasks,
            agent_mapping={"task_a": "mock", "task_b": "mock"},
            input_mappings=input_mappings,
            extract_data=True
        )

        # Validate enhanced prompt for task_b
        task_b_prompt = captured_prompts.get("task_b", "")

        print(f"\nüìù Task B Enhanced Prompt:")
        print("-" * 70)
        print(task_b_prompt[:500] + "..." if len(task_b_prompt) > 500 else task_b_prompt)
        print("-" * 70)

        passed = True

        # Check 1: Prompt was enhanced
        if "CONTEXT FROM UPSTREAM TASKS" not in task_b_prompt:
            print("‚ùå Enhanced prompt missing context section")
            passed = False
        else:
            print("‚úÖ Enhanced prompt contains context section")

        # Check 2: Injected parameters present
        if "target_user" not in task_b_prompt:
            print("‚ùå Missing injected parameter: target_user")
            passed = False
        else:
            print("‚úÖ Injected parameter found: target_user")

        # Check 3: Actual values injected
        if '"root"' not in task_b_prompt and '"alex"' not in task_b_prompt:
            print("‚ùå Injected value not found in prompt")
            passed = False
        else:
            print("‚úÖ Injected value present in prompt")

        if passed:
            print("\n‚úÖ TEST PASSED: Dependency injection validation")
        else:
            print("\n‚ùå TEST FAILED: Dependency injection validation")

        return passed

    except Exception as e:
        print(f"\n‚ùå TEST FAILED: {e}")
        import traceback
        traceback.print_exc()
        return False


# ============================================================================
# Main Test Runner
# ============================================================================

async def main():
    """Run all integration tests"""
    print("\n" + "=" * 70)
    print("DAG + DEPENDENCY INJECTION INTEGRATION TESTS")
    print("=" * 70)

    results = []

    # Test 1: OS User Analysis (3-task chain)
    results.append(("OS User Analysis (Mock)", await test_mock_mode_os_user_analysis()))

    # Test 2: DB Product Sales (2-task chain)
    results.append(("DB Product Sales (Mock)", await test_mock_mode_db_product_sales()))

    # Test 3: Dependency Injection Validation
    results.append(("Dependency Injection Validation", await test_dependency_injection_validation()))

    # Summary
    print("\n" + "=" * 70)
    print("TEST SUMMARY")
    print("=" * 70)

    for test_name, passed in results:
        status = "‚úÖ PASSED" if passed else "‚ùå FAILED"
        print(f"  {status}: {test_name}")

    all_passed = all(passed for _, passed in results)

    if all_passed:
        print("\nüéâ All integration tests passed!")
        print("\n‚úÖ Verified:")
        print("  ‚úì AgentBenchLoader correctly loads dependency task groups")
        print("  ‚úì DAGScheduler executes tasks in topological order")
        print("  ‚úì DependencyInjector enhances prompts with upstream context")
        print("  ‚úì Task results include parsed_data for dependency passing")
        print("  ‚úì Input mappings correctly extract values from upstream tasks")
    else:
        print("\n‚ö†Ô∏è  Some integration tests failed.")

    print("=" * 70 + "\n")

    return all_passed


if __name__ == "__main__":
    try:
        success = asyncio.run(main())
        sys.exit(0 if success else 1)
    except Exception as e:
        print(f"\n‚ùå Integration test failed: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
